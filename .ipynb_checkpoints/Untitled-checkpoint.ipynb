{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing all the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, concatenate, dot, Flatten, Reshape, Bidirectional, add\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import dot, Dot\n",
    "from keras.activations import softmax\n",
    "from keras.layers import Permute, subtract, multiply, GlobalAvgPool1D, GlobalMaxPool1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import Adadelta\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.preprocessing import sequence, text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import functions.utils as utils\n",
    "from functions.utils import *\n",
    "import models.deep_learning as deep_learning\n",
    "from models.deep_learning import *\n",
    "from functions.model_evaluation import *\n",
    "import functions.model_evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(result, id_col, path):\n",
    "    result_final = pd.DataFrame(data = list(zip(id_col, result)), columns = ['id', 'prediction'])\n",
    "    result_final.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the word vector\n",
    "file_name = 'word_embedding/glove.6B.300d.txt'\n",
    "embeddings = load_embedding(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Dataset: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM of Words:4004\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "df = pd.read_csv('Data/train.csv')\n",
    "df['question1'] = df['question1'].apply(str)\n",
    "df['question2'] = df['question2'].apply(str)\n",
    "df.dropna(inplace = True)\n",
    "df = df[:1000]\n",
    "\n",
    "#training and testing\n",
    "seed = 123\n",
    "train, test = train_test_split(df)\n",
    "q = list(train['question1']) + list(train['question2']) + list(test['question1']) + list(test['question2'])\n",
    "#Creating the embedding matrix\n",
    "NUM_WORDS = len(embeddings) #200000\n",
    "tokenize = Tokenizer(num_words = NUM_WORDS)\n",
    "tokenize.fit_on_texts(q)\n",
    "word_index = tokenize.word_index\n",
    "\n",
    "q1_train = tokenize.texts_to_sequences(train['question1'])\n",
    "q2_train = tokenize.texts_to_sequences(train['question2'])\n",
    "q1_test = tokenize.texts_to_sequences(test['question1'])\n",
    "q2_test = tokenize.texts_to_sequences(test['question2'])\n",
    "\n",
    "max_seq_length = max_seq_len(q1_train)\n",
    "max_seq_length = max_seq_len(q2_train, max_seq_length)\n",
    "max_seq_length = max_seq_len(q1_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(q2_test, max_seq_length)\n",
    "\n",
    "q1_train_padded = pad_sequences(q1_train, max_seq_length)\n",
    "q2_train_padded = pad_sequences(q2_train, max_seq_length)\n",
    "q1_test_padded = pad_sequences(q1_test, max_seq_length)\n",
    "q2_test_padded = pad_sequences(q2_test, max_seq_length)\n",
    "\n",
    "#Matrix with the embedding weights\n",
    "embedding_dim = 300\n",
    "embedding_weights = create_embedding_weights(embeddings, embedding_dim, word_index, NUM_WORDS)\n",
    "\n",
    "NUM_WORDS = len(embedding_weights)\n",
    "print(\"NUM of Words:\"+ str(NUM_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = deep_learning.deepModels(embedding_dim = embedding_dim,\n",
    "                embedding_weights = embedding_weights,\n",
    "                max_seq_length = max_seq_length,\n",
    "                NUM_WORDS = NUM_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "batch_size=32\n",
    "epochs=15\n",
    "validation_split=.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 675 samples, validate on 75 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(32, 300), b.shape=(300, 1200), m=32, n=1200, k=300\n\t [[{{node lstm_6_1/while/body/_153/MatMul_1}}]] [Op:__inference_keras_scratch_graph_6105]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-8c1ff82dc2fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                      \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_duplicate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                      validation_split= validation_split)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpred_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq1_test_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2_test_padded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(32, 300), b.shape=(300, 1200), m=32, n=1200, k=300\n\t [[{{node lstm_6_1/while/body/_153/MatMul_1}}]] [Op:__inference_keras_scratch_graph_6105]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "lstm = dl.siamese_lstm()\n",
    "\n",
    "hist_lstm = lstm.fit([q1_train_padded, q2_train_padded],\n",
    "                     train['is_duplicate'],\n",
    "                     batch_size=batch_size, epochs=epochs, \n",
    "                     validation_split= validation_split)\n",
    "\n",
    "pred_lstm = lstm.predict([q1_test_padded, q2_test_padded])\n",
    "\n",
    "result_lstm = []\n",
    "for i in pred_lstm:\n",
    "    if i > .5:\n",
    "        result_lstm.append(1)\n",
    "    else:\n",
    "        result_lstm.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_lstm)):\n",
    "    if result_lstm[i] == test['is_duplicate'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "print(\"SIAMESE LSTM:\")\n",
    "accuracy = (count/len(result_lstm))*100\n",
    "print('accuracy: ' + str(accuracy))\n",
    "print('f1_socre:' + str(f1_score(test['is_duplicate'], result_lstm, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['is_duplicate'], result_lstm, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['is_duplicate'], result_lstm, average = 'weighted')))\n",
    "plot_model_training(hist_lstm)\n",
    "\n",
    "write_csv(result_lstm, test['id'], 'results/deep_learning/quora/lstm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "epochs=15\n",
    "validation_split=.1\n",
    "\n",
    "gru = dl.Siamese_GRU()\n",
    "\n",
    "hist_gru = gru.fit([q1_train_padded, \n",
    "                    q2_train_padded],\n",
    "                   train['Quality'],\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_gru = gru.predict([q1_test_padded, q2_test_padded])\n",
    "\n",
    "result_gru = []\n",
    "for i in pred_gru:\n",
    "    if i > .5:\n",
    "        result_gru.append(1)\n",
    "    else:\n",
    "        result_gru.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_gru)):\n",
    "    if result_gru[i] == test['Quality'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "print(\"SIAMESE GRU:\")\n",
    "accuracy = (count/len(result_gru))*100\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "print('f1_socre:' + str(f1_score(test['is_duplicate'], result_gru, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['is_duplicate'], result_gru, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['is_duplicate'], result_gru, average = 'weighted')))\n",
    "\n",
    "write_csv(result_gru, test['id'], 'results/deep_learning/quora/gru.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "validation_split = 0.1\n",
    "\n",
    "cnn = dl.siamese_cnn(filters= 16, kernel_size= 3)\n",
    "\n",
    "hist_cnn = cnn.fit([q1_train_padded, q2_train_padded], \n",
    "                   train['is_duplicate'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_cnn = cnn.predict([q1_test_padded, q2_test_padded])\n",
    "\n",
    "result_cnn = []\n",
    "for i in pred_cnn:\n",
    "    if i > .5:\n",
    "        result_cnn.append(1)\n",
    "    else:\n",
    "        result_cnn.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_cnn)):\n",
    "    if result_cnn[i] == test['is_duplicate'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_cnn))*100\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['is_duplicate'], result_cnn, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['is_duplicate'], result_cnn, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['is_duplicate'], result_cnn, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_cnn)\n",
    "write_csv(result_cnn, test['id'], 'results/deep_learning/quora/cnn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "validation_split = 0.1\n",
    "\n",
    "snli = dl.deep_nn()\n",
    "\n",
    "hist_snli = snli.fit([q1_train_padded, q2_train_padded], \n",
    "                   train['is_duplicate'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_snli = snli.predict([q1_test_padded, q2_test_padded])\n",
    "\n",
    "result_snli = []\n",
    "for i in pred_snli:\n",
    "    if i > .5:\n",
    "        result_snli.append(1)\n",
    "    else:\n",
    "        result_snli.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_snli)):\n",
    "    if result_snli[i] == test['is_duplicate'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_snli))*100\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['is_duplicate'], result_snli, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['is_duplicate'], result_snli, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['is_duplicate'], result_snli, average = 'weighted')))\n",
    "\n",
    "write_csv(result_snli, test['id'], 'results/deep_learning/quora/deep_nn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "validation_split = 0.1\n",
    "\n",
    "hybrid = dl.hybrid_model()\n",
    "\n",
    "hist_hybrid = hybrid.fit([q1_train_padded, q2_train_padded], \n",
    "                   train['is_duplicate'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_hybrid = hybrid.predict([q1_test_padded, q2_test_padded])\n",
    "\n",
    "result_hybrid = []\n",
    "for i in pred_hybrid:\n",
    "    if i > .5:\n",
    "        result_hybrid.append(1)\n",
    "    else:\n",
    "        result_hybrid.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_hybrid)):\n",
    "    if result_hybrid[i] == test['is_duplicate'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_hybrid))*100\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['is_duplicate'], result_hybrid, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['is_duplicate'], result_hybrid, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['is_duplicate'], result_hybrid, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_hybrid)\n",
    "\n",
    "write_csv(result_hybrid, test['id'], 'results/deep_learning/quora/hybrid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "validation_split = .1\n",
    "\n",
    "e_lstm = dl.enhanced_lstm()\n",
    "\n",
    "hist_e_lstm = e_lstm.fit([q1_train_padded, q2_train_padded], \n",
    "                   train['is_duplicate'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_e_lstm = e_lstm.predict([q1_test_padded, q2_test_padded])\n",
    "\n",
    "result_elstm = []\n",
    "for i in pred_e_lstm:\n",
    "    if i > .5:\n",
    "        result_elstm.append(1)\n",
    "    else:\n",
    "        result_elstm.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_elstm)):\n",
    "    if result_elstm[i] == test['is_duplicate'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_elstm))*100\n",
    "print(\"accuracy: \"  + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['is_duplicate'], result_elstm, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['is_duplicate'], result_elstm, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['is_duplicate'], result_elstm, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_e_lstm)\n",
    "\n",
    "write_csv(result_elstm, test['id'], 'results/deep_learning/quora/elstm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSR Dataset: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16539"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the training and testing data\n",
    "train = pd.read_csv(r'Data/msr_paraphrase_train.txt', sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "test = pd.read_csv(r'Data/msr_paraphrase_test.txt', sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "sent = list(train['#1 String']) + list(train['#2 String']) + list(test['#1 String']) + list(test['#2 String'])\n",
    "\n",
    "#Creating the embedding matrix\n",
    "NUM_WORDS = len(embeddings) #200000\n",
    "tokenize = Tokenizer(num_words = NUM_WORDS)\n",
    "tokenize.fit_on_texts(sent)\n",
    "word_index = tokenize.word_index\n",
    "\n",
    "sent1_train = tokenize.texts_to_sequences(train['#1 String'])\n",
    "sent2_train = tokenize.texts_to_sequences(train['#2 String'])\n",
    "sent1_test = tokenize.texts_to_sequences(test['#1 String'])\n",
    "sent2_test = tokenize.texts_to_sequences(test['#2 String'])\n",
    "\n",
    "max_seq_length = max_seq_len(sent1_train)\n",
    "max_seq_length = max_seq_len(sent2_train, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent1_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent2_test, max_seq_length)\n",
    "\n",
    "sent1_train_padded = pad_sequences(sent1_train, max_seq_length)\n",
    "sent2_train_padded = pad_sequences(sent2_train, max_seq_length)\n",
    "sent1_test_padded = pad_sequences(sent1_test, max_seq_length)\n",
    "sent2_test_padded = pad_sequences(sent2_test, max_seq_length)\n",
    "#Matrix with the embedding weights\n",
    "embedding_dim = 300\n",
    "embedding_weights = create_embedding_weights(embeddings, embedding_dim, word_index, NUM_WORDS)\n",
    "NUM_WORDS = len(embedding_weights)\n",
    "NUM_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = deep_learning.deepModels(embedding_dim = embedding_dim,\n",
    "                embedding_weights = embedding_weights,\n",
    "                max_seq_length = max_seq_length,\n",
    "                NUM_WORDS = NUM_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=15\n",
    "validation_split=.1\n",
    "\n",
    "lstm = dl.siamese_lstm(dropout_lstm=None)\n",
    "\n",
    "hist_lstm = lstm.fit([sent1_train_padded, sent2_train_padded],\n",
    "                     train['Quality'],\n",
    "                     batch_size=batch_size, epochs=epochs, \n",
    "                     validation_split= validation_split)\n",
    "\n",
    "pred_lstm = lstm.predict([sent1_test_padded, sent2_test_padded])\n",
    "\n",
    "result_lstm = []\n",
    "for i in pred_lstm:\n",
    "    if i > .5:\n",
    "        result_lstm.append(1)\n",
    "    else:\n",
    "        result_lstm.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_lstm)):\n",
    "    if result_lstm[i] == test['Quality'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_lstm))*100\n",
    "print('accuracy:'+ str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['Quality'], result_lstm, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['Quality'], result_lstm, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['Quality'], result_lstm, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_lstm)\n",
    "\n",
    "write_csv(result_lstm, test['id'], 'results/deep_learning/MSR/lstm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "epochs=15\n",
    "validation_split=.1\n",
    "\n",
    "gru = dl.Siamese_GRU()\n",
    "\n",
    "hist_gru = gru.fit([sent1_train_padded, \n",
    "                    sent2_train_padded],\n",
    "                   train['Quality'],\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_gru = gru.predict([sent1_test_padded, sent2_test_padded])\n",
    "\n",
    "result_gru = []\n",
    "for i in pred_gru:\n",
    "    if i > .5:\n",
    "        result_gru.append(1)\n",
    "    else:\n",
    "        result_gru.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_gru)):\n",
    "    if result_gru[i] == test['Quality'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_gru))*100\n",
    "print('accuracy: ' + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['Quality'], result_gru, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['Quality'], result_gru, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['Quality'], result_gru, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_gru)\n",
    "\n",
    "write_csv(result_gru, test['id'], 'results/deep_learning/MSR/GRU.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "validation_split = 0.1\n",
    "\n",
    "\n",
    "cnn = dl.siamese_cnn(filters= 16, kernel_size= 3)\n",
    "\n",
    "hist_cnn = cnn.fit([sent1_train_padded, sent2_train_padded], \n",
    "                   train['Quality'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_cnn = cnn.predict([sent1_test_padded, sent2_test_padded])\n",
    "\n",
    "result_cnn = []\n",
    "for i in pred_cnn:\n",
    "    if i > .5:\n",
    "        result_cnn.append(1)\n",
    "    else:\n",
    "        result_cnn.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_cnn)):\n",
    "    if result_cnn[i] == test['Quality'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_cnn))*100\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['Quality'], result_cnn, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['Quality'], result_cnn, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['Quality'], result_cnn, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_cnn)\n",
    "\n",
    "write_csv(result_cnn, test['id'], 'results/deep_learning/MSR/CNN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "validation_split = 0.1\n",
    "\n",
    "snli = dl.deep_nn()\n",
    "\n",
    "hist_snli = snli.fit([sent1_train_padded, sent2_train_padded], \n",
    "                   train['Quality'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_snli = snli.predict([sent1_test_padded, sent2_test_padded])\n",
    "\n",
    "result_snli = []\n",
    "for i in pred_snli:\n",
    "    if i > .5:\n",
    "        result_snli.append(1)\n",
    "    else:\n",
    "        result_snli.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_snli)):\n",
    "    if result_snli[i] == test['Quality'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_snli))*100\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['Quality'], result_snli, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['Quality'], result_snli, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['Quality'], result_snli, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_snli)\n",
    "\n",
    "write_csv(result_snli, test['id'], 'results/deep_learning/MSR/Deep_NN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "validation_split = 0.1\n",
    "\n",
    "hybrid = dl.hybrid_model()\n",
    "\n",
    "hist_hybrid = hybrid.fit([sent1_train_padded, sent2_train_padded], \n",
    "                   train['Quality'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_hybrid = hybrid.predict([sent1_test_padded, sent2_test_padded])\n",
    "\n",
    "result_hybrid = []\n",
    "for i in pred_hybrid:\n",
    "    if i > .5:\n",
    "        result_hybrid.append(1)\n",
    "    else:\n",
    "        result_hybrid.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_hybrid)):\n",
    "    if result_hybrid[i] == test['Quality'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_hybrid))*100\n",
    "print('accuracy: '+ str(accuracy))\n",
    "\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['Quality'], result_hybrid, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['Quality'], result_hybrid, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['Quality'], result_hybrid, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_hybrid)\n",
    "\n",
    "write_csv(result_hybrid, test['id'], 'results/deep_learning/MSR/hybrid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "validation_split = 0.1\n",
    "\n",
    "e_lstm = dl.enhanced_lstm()\n",
    "\n",
    "hist_e_lstm = e_lstm.fit([sent1_train_padded, sent2_train_padded], \n",
    "                   train['Quality'], \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_split=validation_split)\n",
    "\n",
    "pred_e_lstm = e_lstm.predict([sent1_test_padded, sent2_test_padded])\n",
    "\n",
    "result_e_lstm = []\n",
    "for i in pred_e_lstm:\n",
    "    if i > .5:\n",
    "        result_e_lstm.append(1)\n",
    "    else:\n",
    "        result_e_lstm.append(0)\n",
    "        \n",
    "count = 0\n",
    "for i in range(len(result_e_lstm)):\n",
    "    if result_e_lstm[i] == test['Quality'].to_numpy()[i]:\n",
    "        count = count +1\n",
    "\n",
    "accuracy = (count/len(result_e_lstm))*100\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "\n",
    "print('f1_socre:' + str(f1_score(test['Quality'], result_e_lstm, average = 'weighted')))\n",
    "print('recall_socre:' + str(recall_score(test['Quality'], result_e_lstm, average = 'weighted')))\n",
    "print('precision_socre:' + str(precision_score(test['Quality'], result_e_lstm, average = 'weighted')))\n",
    "\n",
    "plot_model_training(hist_e_lstm)\n",
    "\n",
    "write_csv(result_elstm, test['id'], 'results/deep_learning/MSR/elstm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
